{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Hydrofabric Builds MVP\n",
    "\n",
    "This cookbook is an MVP to show a build procedure for getting from the v3 Reference Fabric to a hydrofabric product with Nexus, Flowpath, Divide, and Network Layers\n",
    "\n",
    "The data files in this notebook are preprocessed v3 reference flowpaths and divides that are clipped to a HUC12 basin (010600010202) at Crystal Lake-Collyer Brook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary modules\n",
    "import geopandas as gpd\n",
    "\n",
    "from examples.cookbook.workflow import (\n",
    "    aggregate_geometries_from_pairs_and_groups,\n",
    "    aggregate_with_all_rules,\n",
    "    build_hydroseq_network,\n",
    "    find_outlets_by_hydroseq,\n",
    "    reindex_layers_with_topology,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all references used in this proof of concept\n",
    "huc_gdf = gpd.read_file(\"sample_huc.gpkg\")\n",
    "fp_gdf = gpd.read_file(\"sample_flowpaths.gpkg\")\n",
    "div_gdf = gpd.read_file(\"sample_divides.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the outputs to visualize the outputs of our reference\n",
    "m = huc_gdf.explore(color=\"black\", fill=False, weight=5)\n",
    "m = div_gdf.explore(m=m, color=\"red\", alpha=0.2)\n",
    "fp_gdf.explore(m=m, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Step 1: Create a Network Structure\n",
    "\n",
    "First we will need to determine individual networks within our reference and construct a graph oriented object to connect flowpaths to their upstream neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_hydroseq_network(fp_gdf)\n",
    "outlets = find_outlets_by_hydroseq(fp_gdf)\n",
    "\n",
    "print(f\"Found {len(outlets)} outlets\")\n",
    "print(f\"Outlet: {outlets}\")\n",
    "\n",
    "print(\"Network Object:\")\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Step 2: Refactoring of the network to determine divides to be aggregated\n",
    "\n",
    "Once the graph structure is created for the reference flowpaths, small flowpaths and divides need to be aggregated to better support routing stability. Currently, we are enforcing the following rules when aggreagating flowpaths:\n",
    "\n",
    "#### Reference flowpath network consistency assumptions\n",
    "- Hydroseq Reliability: The code assumes hydroseq/dnhydroseq fields provide accurate network topology. Lower hydroseq values indicate more downstream position, with differences reflecting true flow direction.\n",
    "- Complete Network Coverage: All flowpaths in the dataset form a connected network graph. Missing or invalid dnhydroseq values indicate either outlets or data quality issues.\n",
    "\n",
    "### Aggregation Rules\n",
    "- Rule Priority Order: Algorithm applies rules in strict sequence: (1) 4km segment length Independence, (2) Small Catchment Aggregation ( < 0.1km2), (3) Stream Order-1 Branch Aggregation, (4) Drainage Area Aggregation. First applicable rule takes precedence.\n",
    "- Length Threshold Absolute: Flowpaths ≥4km length ALWAYS remain independent, regardless of other characteristics. This represents a minimum modeling unit size requirement.\n",
    "- Small Catchment Definition: flowpaths with areasqkm_left <0.1km² are considered \"small catchments\" requiring special aggregation treatment to prevent loss of hydrologic significance.\n",
    "- Stream Order Hierarchy: Order-2 streams are preferred aggregation targets over Order-1 when available. Order-1 streams aggregate their entire upstream branch as a headwater unit.\n",
    "- Drainage Area Dominance: When no order-specific rules apply, flowpaths aggregate with their largest upstream tributary by total drainage area (totdasqkm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length_threshold = 4.0\n",
    "small_catchment_threshold = 0.1\n",
    "all_aggregation_pairs = []\n",
    "all_headwater_groups = []\n",
    "all_independent_flowpaths = []\n",
    "all_minor_flowpaths = []\n",
    "\n",
    "for outlet in outlets:\n",
    "    print(f\"  Processing outlet {outlet}...\")\n",
    "    result = aggregate_with_all_rules(\n",
    "        network_graph=network,\n",
    "        fp=fp_gdf,\n",
    "        start_id=outlet,\n",
    "        segment_length_threshold=segment_length_threshold,\n",
    "        small_catchment_threshold=small_catchment_threshold,\n",
    "    )\n",
    "\n",
    "    all_aggregation_pairs.extend(result[\"aggregation_pairs\"])\n",
    "    all_headwater_groups.extend(result[\"headwater_groups\"])\n",
    "    all_independent_flowpaths.extend(result[\"independent_flowpaths\"])\n",
    "    all_minor_flowpaths.extend(result[\"minor_flowpaths\"])\n",
    "\n",
    "print(\"\\nTotal aggregation relationships identified:\")\n",
    "print(f\"  Pairs: {len(all_aggregation_pairs)}\")\n",
    "print(f\"  Headwater groups: {len(all_headwater_groups)}\")\n",
    "print(f\"  Independent: {len(all_independent_flowpaths)}\")\n",
    "print(f\"  Flowlines: {len(all_minor_flowpaths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Step 3: Aggregate Geometries \n",
    "\n",
    "This step takes any flowpath items that need to be combined and aggregates their geometries together into a single shape. This is important for divides with zonal statistic calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_result = aggregate_geometries_from_pairs_and_groups(\n",
    "    flowpaths_gdf=fp_gdf,\n",
    "    divides_gdf=div_gdf,\n",
    "    aggregation_pairs=all_aggregation_pairs,\n",
    "    headwater_groups=all_headwater_groups,\n",
    "    independent_flowpaths=all_independent_flowpaths,\n",
    "    minor_flowpaths=all_minor_flowpaths,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Step 4: Re-indexing flowpaths and divides and add nexus creation\n",
    "\n",
    "Once geometries are reaggregated, a nexus-topology and re-indexing has to be performed to connect catchments -> flowpaths as a 1:1 reference and creates nexus points for flow aggregation. The network table is also created.\n",
    "\n",
    "*NOTE:* Flowlines (which we call minor flowpaths to reduce confusion) are not included in the flowpaths layer as the v3 prototype Hydrofabric layers in VPU01 did not have these flowlines included. This capability is meant to use the v3 prototype Hydrofabric as a reference\n",
    "\n",
    "*NOTE:* NHD references through `hf_id` were not included as the NHD dataset is too large to fit in a notebook and ship with the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = reindex_layers_with_topology(fp_gdf, div_gdf, geometry_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Let's look at our created Hydrofabric \n",
    "\n",
    "Now that we've run all of the end -> end steps, let's take a look at each of the required layers. The database schema of each layer is intended to match that used by the prototype v3 Hydrofabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "hf[\"network\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides\n",
    "hf[\"divides\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowpaths\n",
    "hf[\"flowpaths\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nexus\n",
    "hf[\"nexus\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the outputs on a map\n",
    "m = huc_gdf.explore(color=\"black\", fill=False, weight=5)\n",
    "m = hf[\"divides\"].explore(m=m, color=\"red\", alpha=0.2)\n",
    "m = hf[\"flowpaths\"].explore(m=m, color=\"blue\")\n",
    "hf[\"nexus\"].explore(m=m, color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also save the outputs to disk for external verification if necessary\n",
    "output_file = \"MVP_NGWPC_hydrofabric.gpkg\"\n",
    "for table_name, _layer in hf.items():\n",
    "    if len(_layer) > 0:\n",
    "        gpd.GeoDataFrame(_layer).to_file(output_file, layer=table_name, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Warning: {table_name} layer is empty\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
