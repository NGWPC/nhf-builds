{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Download and process GLIMS glaciers\n",
    "\n",
    "GLIMS is a multi-temporal worldwide glacier dataset. Here, we download the most recent GLIMS file from NASA Earth Data and filter it to the USA for use in Hydrofabric.\n",
    "\n",
    "GLIMS glaciers are stored on NASA's earth data server. You will need an earthdata login to download. This notebook will programmatically download the most recent GLIMS file.\n",
    "\n",
    "https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0272_GLIMS_v1/ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shlex\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "from dateutil import parser\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=Path(\"..\") / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/glims\")\n",
    "usa_file = data_path / \"tl_2024_us_state.gpkg\"  # You can download this from s3://edfs-data/boundaries\n",
    "out_parquet_file = data_path / \"glims_us_20250624.parquet\"\n",
    "out_usa_buffer = data_path / \"temp_usa_buffer.gpkg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Download\n",
    "\n",
    "Follow instructions to store your NASA earthdata login and ID in `~/.netc`\n",
    "\n",
    "`echo 'machine urs.earthdata.nasa.gov login <login> password <password>' >> ~/.netrc`\n",
    "\n",
    "https://nsidc.org/data/user-resources/help-center/programmatic-access-guide-data-daacdataapps\n",
    "\n",
    "Sample wget:\n",
    "```\n",
    "wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies --no-check-certificate --auth-no-challenge=on -r --reject \"index.html*\" -np -e robots=off https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0272_GLIMS_v1/NSIDC-0272_glims_db_north_20230607_v01.0.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use spider to get a list of available files on the server\n",
    "# if you're having trouble, try downloading an arbirtary file with the sample wget before to load cookies\n",
    "!wget --spider -r --no-parent --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies --no-check-certificate --auth-no-challenge=on https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0272_GLIMS_v1/ 2>&1 | grep -o 'https://[^ ]*' > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open list of files retrieved\n",
    "with open(\"output.txt\") as f:\n",
    "    file_list = f.readlines()\n",
    "\n",
    "# get the most recent date\n",
    "date_list = []\n",
    "dt = datetime(1900, 1, 1)  # starting date - old\n",
    "\n",
    "for _i, f in enumerate(file_list):\n",
    "    # find date\n",
    "    dt_txt = re.findall(r\"\\d{4}\\d{2}\\d{2}\", f)\n",
    "    if dt_txt:\n",
    "        this_dt = parser.parse(dt_txt[0])\n",
    "        date_list.append([this_dt, f])\n",
    "        # if date is greater, save it as greatest\n",
    "        if this_dt > dt:\n",
    "            dt = this_dt\n",
    "\n",
    "# get files that have the latest date where date_list is [[date, file], [date, file]]\n",
    "# we want the north .zip (no .md5)\n",
    "to_download = []\n",
    "for i, f in enumerate(date_list):\n",
    "    if dt == f[0]:\n",
    "        if (\"north\" in f[1]) and (\".md5\" not in f[1]):\n",
    "            to_download.append(date_list[i][1])\n",
    "\n",
    "final_download = to_download[0].strip(\"\\n\")  # newline was incorporated in txt\n",
    "print(final_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the download\n",
    "subprocess.run(\n",
    "    shlex.split(\n",
    "        f\"wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies --no-check-certificate --auth-no-challenge=on -r --reject 'index.html*' -np -e robots=off -nd {final_download}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the download - it downloads to the current directory\n",
    "with ZipFile(final_download.split(\"/\")[-1]) as z:\n",
    "    z.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory has a random number of the download in it, so search for it, (ex. glims_download_35992)\n",
    "dirs = []\n",
    "for f in data_path.iterdir():\n",
    "    dirs.append(f)\n",
    "glims_poly = dirs[0] / \"glims_polygons.shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Process GLIMS\n",
    "\n",
    "GLIMS polygons have broken geometries that need to be repaired. \n",
    "\n",
    "To lower file size and speed up later operations with hydrofabric, we'll extract out a buffered USA.\n",
    "\n",
    "To buffer the USA, download the tiger line file from test account s3: \n",
    "\n",
    "`s3://edfs-data/boundaries/tl_2024_us_state.gpkg`\n",
    "\n",
    "Store it in the working data folder for local use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(glims_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repair geometry\n",
    "gdf[\"geometry\"] = gdf[\"geometry\"].make_valid()\n",
    "\n",
    "# extract existing glaciers\n",
    "gdf_exists = gdf.loc[gdf[\"glac_stat\"] == \"exists\", :].copy()\n",
    "gdf_exists.to_file(data_path / \"temp_glims_exists_geom_repair.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read US, exclude unneeded states, dissolve to one polygon, buffer to include canada watersheds\n",
    "gdf_usa = gpd.read_file(usa_file)\n",
    "gdf_usa = gdf_usa.loc[~gdf_usa[\"STUSPS\"].isin([\"MP\", \"AS\", \"GU\"]), :]\n",
    "gdf_usa_dissolve = gdf_usa.dissolve()\n",
    "gdf_usa_dissolve = gdf_usa_dissolve.to_crs(5070)\n",
    "gdf_usa_dissolve = gdf_usa_dissolve.buffer(200000)\n",
    "gdf_usa_dissolve = gpd.GeoDataFrame(geometry=gdf_usa_dissolve, data={\"temp\": [\"buffer\"]})\n",
    "gdf_usa_dissolve.to_file(out_usa_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect glaciers with USA polygon\n",
    "# can take a few min\n",
    "gdf_usa_dissolve = gdf_usa_dissolve.to_crs(gdf_exists.crs)\n",
    "gdf_int = gdf_exists.overlay(gdf_usa_dissolve, how=\"intersection\")\n",
    "gdf_int.to_parquet(out_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to parquet and zip file to s3 for storage\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=os.environ[\"AWS_SESSION_TOKEN\"],\n",
    ")\n",
    "s3_client.upload_file(out_parquet_file, \"edfs-data\", \"glaciers/glims_20250624.parquet\")\n",
    "\n",
    "zip_file = final_download.split(\"/\")[-1]\n",
    "s3_client.upload_file(zip_file, \"edfs-data\", f\"glaciers/{zip_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrofabric-builds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
