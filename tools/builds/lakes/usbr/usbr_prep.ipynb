{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Download and process USBR reservoirs\n",
    "\n",
    "The following notebook provides code to scrape the USBR reservoir data in order to find what USBR reservoirs are in the hydrofabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Changes the current working dir to be the project root\n",
    "current_working_dir = Path.cwd()\n",
    "os.chdir(current_working_dir / \"../../../../\")\n",
    "print(\n",
    "    f\"Changed current working dir from {current_working_dir} to: {Path.cwd()}. This notebook must run at the project root\"\n",
    ")\n",
    "\n",
    "# dir is where the .env file is located\n",
    "load_dotenv(dotenv_path=Path.cwd())\n",
    "pyiceberg_file = Path.cwd() / \".pyiceberg.yaml\"\n",
    "if pyiceberg_file.exists():\n",
    "    os.environ[\"PYICEBERG_HOME\"] = str(pyiceberg_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find .pyiceberg.yaml. Please download this from NGWPC confluence or create \"\n",
    "    )\n",
    "\n",
    "# Loading the local pyiceberg config settings\n",
    "try:\n",
    "    with open(Path.cwd() / \".pyiceberg.yaml\", encoding=\"utf-8\") as file:\n",
    "        pyiceberg_config = yaml.safe_load(file)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\".pyiceberg YAML file not found in cwd: {Path.cwd() / '../../'}\") from e\n",
    "except yaml.YAMLError as e:\n",
    "    raise yaml.YAMLError(f\"Error parsing .pyiceberg YAML file: {e}\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Get all of the USBR data. For more inforrmation on the endpoint, see https://data.usbr.gov/rise/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "\"\"\"Get all reservoir locations from USBR RISE API\"\"\"\n",
    "base_url = \"https://data.usbr.gov/rise/api/location\"\n",
    "all_data = []\n",
    "page = 1\n",
    "\n",
    "print(\"Retrieving USBR locations...\")\n",
    "\n",
    "with httpx.Client(timeout=30.0) as client:\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.get(\n",
    "                base_url,\n",
    "                params={\"page\": page, \"itemsPerPage\": 100},\n",
    "                headers={\"Accept\": \"application/vnd.api+json\"},\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            content = response.json()\n",
    "            data_page = content.get(\"data\", [])\n",
    "\n",
    "            if not data_page:\n",
    "                break\n",
    "\n",
    "            all_data.extend(data_page)\n",
    "            print(f\"Retrieved page {page}, total entries: {len(all_data)}\")\n",
    "            page += 1\n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"Error retrieving page {page}: {e}\")\n",
    "            break\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error retrieving page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "print(f\"Total entries retrieved: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Process all data from the requests and format them into a list based on which are lakes/reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Process the data\n",
    "location_type = \"Lake/Reservoir\"  # ensuring we only are about lakes and reservoirs\n",
    "locations = []\n",
    "location_types = {}\n",
    "\n",
    "for _i, item in enumerate(all_data):\n",
    "    attrs = item.get(\"attributes\", {})\n",
    "    coords_data = attrs.get(\"locationCoordinates\", {})\n",
    "    coords = coords_data.get(\"coordinates\") if coords_data else None\n",
    "\n",
    "    loc_type = attrs.get(\"locationTypeName\")\n",
    "    if loc_type:\n",
    "        location_types[loc_type] = location_types.get(loc_type, 0) + 1\n",
    "\n",
    "    if coords and len(coords) == 2:\n",
    "        locations.append(\n",
    "            {\n",
    "                \"locationName\": attrs.get(\"locationName\"),\n",
    "                \"longitude\": coords[0],\n",
    "                \"latitude\": coords[1],\n",
    "                \"locationType\": loc_type,\n",
    "                \"location_id\": attrs.get(\"_id\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"\\nLocation types found:\")\n",
    "for loc_type, count in sorted(location_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {loc_type}: {count}\")\n",
    "\n",
    "# Convert to DataFrame and filter\n",
    "reservoirs_df = pd.DataFrame(locations)\n",
    "\n",
    "if location_type:\n",
    "    reservoirs_df = reservoirs_df[reservoirs_df[\"locationType\"] == location_type].copy()\n",
    "    print(f\"\\nFiltered to {len(reservoirs_df)} '{location_type}' entries\")\n",
    "\n",
    "reservoirs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Read in the hydrofabric in order to determine which lakes are already represented in the hydrofabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def to_geopandas(df: pd.DataFrame, crs: str = \"EPSG:5070\") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Converts the geometries in a pandas df to a geopandas dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The iceberg table you are trying to read from\n",
    "    crs: str, optional\n",
    "        A string representing the CRS to set in the gdf, by default \"EPSG:5070\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.DataFrame\n",
    "        The resulting queried row, but in a geodataframe\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        Raised if the table does not have a geometry column\n",
    "    \"\"\"\n",
    "    if \"geometry\" not in df.columns:\n",
    "        raise ValueError(\"The provided table does not have a geometry column.\")\n",
    "\n",
    "    return gpd.GeoDataFrame(df, geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]), crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "# Loading SQL Catalog\n",
    "# This catalog can be downloaded through the icefabric repo. We care about the conus_hf namespace\n",
    "catalog = load_catalog(\n",
    "    name=\"sql\",\n",
    "    type=pyiceberg_config[\"catalog\"][\"sql\"][\"type\"],\n",
    "    uri=pyiceberg_config[\"catalog\"][\"sql\"][\"uri\"],\n",
    "    warehouse=pyiceberg_config[\"catalog\"][\"sql\"][\"warehouse\"],\n",
    ")\n",
    "\n",
    "# Loading Glue Catalog\n",
    "# catalog = load_catalog(\"glue\", **{\n",
    "#     \"type\": \"glue\",\n",
    "#     \"glue.region\": \"us-east-1\"\n",
    "# })\n",
    "\n",
    "lakes = to_geopandas(catalog.load_table(\"conus_hf.lakes\").scan().to_pandas())\n",
    "divides = to_geopandas(catalog.load_table(\"conus_hf.divides\").scan().to_pandas())\n",
    "hydrolocations = catalog.load_table(\"conus_hf.hydrolocations\").scan().to_pandas()\n",
    "pois = catalog.load_table(\"conus_hf.pois\").scan().to_pandas()\n",
    "network = catalog.load_table(\"conus_hf.network\").scan().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "# Create reservoir points\n",
    "geometry = [Point(xy) for xy in zip(reservoirs_df.longitude, reservoirs_df.latitude, strict=True)]\n",
    "reservoirs_gdf = gpd.GeoDataFrame(reservoirs_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure matching CRS. This should be EPSG:5070\n",
    "if reservoirs_gdf.crs != lakes.crs:\n",
    "    print(f\"Reprojecting reservoirs from {reservoirs_gdf.crs} to {lakes.crs}\")\n",
    "    reservoirs_gdf = reservoirs_gdf.to_crs(lakes.crs)\n",
    "\n",
    "# Buffer the reservoir points\n",
    "buffer_meters = 1000  # Adjust this value as needed\n",
    "print(f\"Applying {buffer_meters}m buffer to reservoir points...\")\n",
    "\n",
    "# Apply buffer in projected coordinates\n",
    "reservoirs_buffered = reservoirs_gdf.copy()\n",
    "reservoirs_buffered.geometry = reservoirs_buffered.geometry.buffer(buffer_meters)\n",
    "\n",
    "# Spatial join\n",
    "print(\"Performing spatial join...\")\n",
    "matches = gpd.sjoin(reservoirs_buffered, lakes, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "print(f\"Found {len(matches)} reservoir-lake matches\")\n",
    "print(f\"Matched {matches['locationName'].nunique()} unique reservoirs\")\n",
    "\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrolocations_first = hydrolocations.drop_duplicates(subset=[\"poi_id\"], keep=\"first\")\n",
    "\n",
    "matches_extended = pd.merge(\n",
    "    matches,\n",
    "    hydrolocations_first,\n",
    "    how=\"inner\",\n",
    "    on=\"poi_id\",  # or whatever your join column is\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_extended.to_parquet(\"data/usbr/reservoir_matches.parquet\")\n",
    "matches_extended.to_file(\"data/usbr/reservoir_matches.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
